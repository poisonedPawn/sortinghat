{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "epochs = 20\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_label = {\n",
    "    \"Gryffindor\":0,\n",
    "    \"Hufflepuff\":1,\n",
    "    \"Ravenclaw\":2,\n",
    "    \"Slytherin\":3     \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = pd.read_csv(\"pics_by_house64x.csv\", index_col=0)\n",
    "\n",
    "X = full_dataset.drop(columns=['house'])\n",
    "X = torch.tensor(X.values).float()\n",
    "y = full_dataset['house'].apply(lambda x: house_label[x])\n",
    "y = torch.tensor(y.values)\n",
    "\n",
    "train_size, test_size, validation_size = 0.8, 0.1, 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self, samples,labels):\n",
    "        self.samples = samples\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = myDataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, validation_data = torch.utils.data.random_split(\n",
    "    data,\n",
    "    [\n",
    "        int(train_size*full_dataset.shape[0]),\n",
    "        int(test_size*full_dataset.shape[0]),\n",
    "        full_dataset.shape[0]-(int(test_size*full_dataset.shape[0])+int(train_size*full_dataset.shape[0]))\n",
    "    ],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.myDataset at 0x215f9eeb8b0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=batch_size, shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder_hidden_layer = nn.Linear(\n",
    "            in_features=kwargs[\"input_shape\"], out_features=512\n",
    "        )\n",
    "        self.encoder_output_layer = nn.Linear(\n",
    "            in_features=512, out_features=128\n",
    "        )\n",
    "        self.decoder_hidden_layer = nn.Linear(\n",
    "            in_features=128, out_features=512\n",
    "        )\n",
    "        self.decoder_output_layer = nn.Linear(\n",
    "            in_features=512, out_features=kwargs[\"input_shape\"]\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        activation = self.encoder_hidden_layer(features)\n",
    "        activation = torch.relu(activation)\n",
    "        code = self.encoder_output_layer(activation)\n",
    "        code = torch.sigmoid(code)\n",
    "        activation = self.decoder_hidden_layer(code)\n",
    "        activation = torch.relu(activation)\n",
    "        activation = self.decoder_output_layer(activation)\n",
    "        reconstructed = torch.sigmoid(activation)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# create a model from `AE` autoencoder class\n",
    "# load it to the specified device, either gpu or cpu\n",
    "model = AE(input_shape=4096).to(device)\n",
    "\n",
    "# create an optimizer object\n",
    "# Adam optimizer with learning rate 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# mean-squared error loss\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[126., 131., 144.,  ..., 106., 106., 125.],\n",
      "        [125., 138., 144.,  ...,  95.,  91., 116.],\n",
      "        [111.,  91., 103.,  ...,  92.,  92.,  98.],\n",
      "        ...,\n",
      "        [128., 127., 128.,  ..., 139., 130., 125.],\n",
      "        [131., 137., 134.,  ...,  81.,  64.,  92.],\n",
      "        [127., 135., 144.,  ..., 136., 143., 135.]], device='cuda:0')\n",
      "tensor([[0.4785, 0.5002, 0.5917,  ..., 0.4923, 0.5009, 0.5174],\n",
      "        [0.4995, 0.5220, 0.5594,  ..., 0.5033, 0.4682, 0.5151],\n",
      "        [0.4996, 0.5145, 0.5842,  ..., 0.4996, 0.4651, 0.5406],\n",
      "        ...,\n",
      "        [0.4973, 0.5070, 0.5753,  ..., 0.4736, 0.4875, 0.5268],\n",
      "        [0.5035, 0.5124, 0.5689,  ..., 0.4953, 0.4593, 0.5165],\n",
      "        [0.4772, 0.5071, 0.5934,  ..., 0.5002, 0.4795, 0.5263]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor(19545.5684, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "19545.568359375\n",
      "epoch : 1/20, recon loss = 19545.56835938\n",
      "tensor([[115.,  94.,  98.,  ...,  87.,  82., 105.],\n",
      "        [160., 197., 188.,  ..., 222., 217., 210.],\n",
      "        [127., 129., 140.,  ..., 126., 122., 127.],\n",
      "        ...,\n",
      "        [107.,  75.,  77.,  ...,  50.,  43.,  77.],\n",
      "        [146., 178., 172.,  ..., 193., 212., 182.],\n",
      "        [139., 142., 147.,  ..., 119.,  92.,  87.]], device='cuda:0')\n",
      "tensor([[0.4445, 0.5532, 0.5914,  ..., 0.5165, 0.4991, 0.5960],\n",
      "        [0.4446, 0.5509, 0.5877,  ..., 0.5097, 0.4980, 0.5977],\n",
      "        [0.4451, 0.5549, 0.5937,  ..., 0.5212, 0.5003, 0.5942],\n",
      "        ...,\n",
      "        [0.4451, 0.5549, 0.5937,  ..., 0.5212, 0.5003, 0.5942],\n",
      "        [0.4451, 0.5549, 0.5937,  ..., 0.5212, 0.5003, 0.5942],\n",
      "        [0.4451, 0.5548, 0.5936,  ..., 0.5211, 0.5002, 0.5943]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor(19538.8066, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "19538.806640625\n",
      "epoch : 2/20, recon loss = 19538.80664062\n",
      "tensor([[129., 129., 128.,  ..., 139., 130., 126.],\n",
      "        [131., 144., 137.,  ...,  61.,  52.,  84.],\n",
      "        [126., 133., 150.,  ..., 105., 106., 125.],\n",
      "        ...,\n",
      "        [125., 137., 150.,  ...,  99.,  99., 121.],\n",
      "        [127., 128., 126.,  ..., 142., 144., 134.],\n",
      "        [143.,  89., 115.,  ..., 201., 201., 201.]], device='cuda:0')\n",
      "tensor([[0.4858, 0.5954, 0.6135,  ..., 0.5702, 0.5297, 0.6049],\n",
      "        [0.4789, 0.5933, 0.6134,  ..., 0.5588, 0.5264, 0.6060],\n",
      "        [0.4819, 0.5957, 0.6136,  ..., 0.5669, 0.5243, 0.6064],\n",
      "        ...,\n",
      "        [0.4773, 0.5934, 0.6130,  ..., 0.5566, 0.5237, 0.6069],\n",
      "        [0.4811, 0.5958, 0.6138,  ..., 0.5665, 0.5235, 0.6066],\n",
      "        [0.4746, 0.5937, 0.6133,  ..., 0.5547, 0.5204, 0.6084]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor(19531.5449, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "19531.544921875\n",
      "epoch : 3/20, recon loss = 19531.54492188\n",
      "tensor([[ 77.,  62.,  57.,  ...,  68.,  55.,  68.],\n",
      "        [ 93.,  59.,  63.,  ..., 252., 255., 214.],\n",
      "        [ 62.,  61.,  61.,  ...,  37.,  66.,  63.],\n",
      "        ...,\n",
      "        [128., 129., 127.,  ..., 130., 128., 129.],\n",
      "        [117., 103., 106.,  ..., 104., 102., 107.],\n",
      "        [103.,  79.,  92.,  ...,  36.,  38.,  45.]], device='cuda:0')\n",
      "tensor([[0.4869, 0.6412, 0.6829,  ..., 0.6092, 0.5651, 0.6591],\n",
      "        [0.4869, 0.6412, 0.6829,  ..., 0.6092, 0.5651, 0.6591],\n",
      "        [0.4869, 0.6412, 0.6829,  ..., 0.6092, 0.5651, 0.6591],\n",
      "        ...,\n",
      "        [0.4869, 0.6412, 0.6829,  ..., 0.6092, 0.5651, 0.6590],\n",
      "        [0.4869, 0.6412, 0.6829,  ..., 0.6092, 0.5651, 0.6591],\n",
      "        [0.4869, 0.6412, 0.6829,  ..., 0.6092, 0.5651, 0.6591]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor(19521.6602, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "19521.66015625\n",
      "epoch : 4/20, recon loss = 19521.66015625\n",
      "tensor([[128., 132., 133.,  ..., 121., 100., 119.],\n",
      "        [  5.,   4.,   5.,  ...,  17.,  17.,  17.],\n",
      "        [102.,  70.,  77.,  ...,  56.,  47.,  80.],\n",
      "        ...,\n",
      "        [123., 136., 141.,  ..., 190., 187., 189.],\n",
      "        [ 26.,  14.,  13.,  ...,  16.,  38.,  56.],\n",
      "        [124., 111., 112.,  ...,  75., 117., 131.]], device='cuda:0')\n",
      "tensor([[0.5289, 0.6883, 0.7257,  ..., 0.6566, 0.6079, 0.6940],\n",
      "        [0.5289, 0.6883, 0.7257,  ..., 0.6566, 0.6079, 0.6940],\n",
      "        [0.5289, 0.6883, 0.7257,  ..., 0.6566, 0.6079, 0.6940],\n",
      "        ...,\n",
      "        [0.5289, 0.6883, 0.7257,  ..., 0.6566, 0.6079, 0.6940],\n",
      "        [0.5289, 0.6883, 0.7257,  ..., 0.6566, 0.6079, 0.6940],\n",
      "        [0.5289, 0.6883, 0.7257,  ..., 0.6566, 0.6079, 0.6940]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor(19510.5488, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "19510.548828125\n",
      "epoch : 5/20, recon loss = 19510.54882812\n",
      "tensor([[128., 132., 133.,  ..., 121., 100., 119.],\n",
      "        [ 52.,  19.,  69.,  ..., 147., 151., 146.],\n",
      "        [127., 129., 140.,  ..., 128., 128., 129.],\n",
      "        ...,\n",
      "        [135., 146., 147.,  ..., 139., 135., 131.],\n",
      "        [ 36.,  18.,  16.,  ...,  49.,  75.,  71.],\n",
      "        [123., 116., 127.,  ..., 140., 147., 140.]], device='cuda:0')\n",
      "tensor([[0.5743, 0.7407, 0.7701,  ..., 0.7029, 0.6716, 0.7433],\n",
      "        [0.5743, 0.7407, 0.7701,  ..., 0.7029, 0.6716, 0.7433],\n",
      "        [0.5743, 0.7407, 0.7701,  ..., 0.7029, 0.6716, 0.7433],\n",
      "        ...,\n",
      "        [0.5743, 0.7407, 0.7701,  ..., 0.7029, 0.6716, 0.7433],\n",
      "        [0.5743, 0.7407, 0.7701,  ..., 0.7029, 0.6716, 0.7433],\n",
      "        [0.5743, 0.7407, 0.7701,  ..., 0.7029, 0.6716, 0.7433]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor(19497.6777, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "19497.677734375\n",
      "epoch : 6/20, recon loss = 19497.67773438\n",
      "tensor([[127., 131., 138.,  ..., 140., 142., 131.],\n",
      "        [124., 113., 115.,  ...,  70.,  89., 120.],\n",
      "        [128., 127., 128.,  ..., 139., 130., 125.],\n",
      "        ...,\n",
      "        [116.,  98., 104.,  ...,  48., 170., 200.],\n",
      "        [126., 133., 143.,  ..., 114., 112., 125.],\n",
      "        [121., 103., 106.,  ...,  62.,  48.,  79.]], device='cuda:0')\n",
      "tensor([[0.6310, 0.7948, 0.8192,  ..., 0.7592, 0.7300, 0.7874],\n",
      "        [0.6310, 0.7948, 0.8192,  ..., 0.7592, 0.7300, 0.7874],\n",
      "        [0.6310, 0.7948, 0.8192,  ..., 0.7592, 0.7300, 0.7874],\n",
      "        ...,\n",
      "        [0.6310, 0.7948, 0.8192,  ..., 0.7592, 0.7300, 0.7874],\n",
      "        [0.6310, 0.7948, 0.8192,  ..., 0.7592, 0.7300, 0.7874],\n",
      "        [0.6310, 0.7948, 0.8192,  ..., 0.7592, 0.7300, 0.7874]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor(19483.9805, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "19483.98046875\n",
      "epoch : 7/20, recon loss = 19483.98046875\n",
      "tensor([[129., 136., 129.,  ...,  78.,  67., 103.],\n",
      "        [100.,  70.,  75.,  ...,  62.,  49.,  77.],\n",
      "        [129., 125., 123.,  ..., 129., 132., 131.],\n",
      "        ...,\n",
      "        [128., 128., 136.,  ..., 112., 111., 124.],\n",
      "        [ 16.,   9.,  26.,  ...,  23.,  22.,  15.],\n",
      "        [127., 128., 126.,  ..., 142., 144., 134.]], device='cuda:0')\n",
      "tensor([[0.6904, 0.8447, 0.8663,  ..., 0.8112, 0.7924, 0.8367],\n",
      "        [0.6904, 0.8447, 0.8663,  ..., 0.8112, 0.7924, 0.8367],\n",
      "        [0.6904, 0.8447, 0.8663,  ..., 0.8112, 0.7924, 0.8367],\n",
      "        ...,\n",
      "        [0.6904, 0.8447, 0.8663,  ..., 0.8112, 0.7924, 0.8367],\n",
      "        [0.6904, 0.8447, 0.8663,  ..., 0.8112, 0.7924, 0.8367],\n",
      "        [0.6904, 0.8447, 0.8663,  ..., 0.8112, 0.7924, 0.8367]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor(19470.3633, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "19470.36328125\n",
      "epoch : 8/20, recon loss = 19470.36328125\n",
      "tensor([[127., 122., 123.,  ..., 175., 173., 145.],\n",
      "        [134., 149., 142.,  ...,  52.,  49.,  81.],\n",
      "        [129., 123., 127.,  ..., 189., 107.,  88.],\n",
      "        ...,\n",
      "        [119.,  98., 100.,  ...,  85.,  81.,  89.],\n",
      "        [128., 129., 129.,  ..., 152., 139., 125.],\n",
      "        [121., 103., 106.,  ...,  77.,  79., 103.]], device='cuda:0')\n",
      "tensor([[0.7529, 0.8889, 0.9066,  ..., 0.8598, 0.8500, 0.8816],\n",
      "        [0.7529, 0.8889, 0.9066,  ..., 0.8598, 0.8500, 0.8816],\n",
      "        [0.7529, 0.8889, 0.9066,  ..., 0.8598, 0.8500, 0.8816],\n",
      "        ...,\n",
      "        [0.7529, 0.8889, 0.9066,  ..., 0.8598, 0.8500, 0.8816],\n",
      "        [0.7529, 0.8889, 0.9066,  ..., 0.8598, 0.8500, 0.8816],\n",
      "        [0.7529, 0.8889, 0.9066,  ..., 0.8598, 0.8500, 0.8816]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor(19457.6777, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "19457.677734375\n",
      "epoch : 9/20, recon loss = 19457.67773438\n",
      "tensor([[137., 147., 144.,  ...,  81.,  65.,  85.],\n",
      "        [ 98.,  64.,  75.,  ..., 132., 103., 101.],\n",
      "        [130., 132., 128.,  ...,  71.,  67., 105.],\n",
      "        ...,\n",
      "        [125., 138., 144.,  ...,  95.,  91., 116.],\n",
      "        [119., 110., 115.,  ..., 159., 164., 157.],\n",
      "        [125., 116., 116.,  ..., 100.,  69.,  89.]], device='cuda:0')\n",
      "tensor([[0.8139, 0.9248, 0.9381,  ..., 0.9015, 0.8980, 0.9191],\n",
      "        [0.8139, 0.9248, 0.9381,  ..., 0.9015, 0.8980, 0.9191],\n",
      "        [0.8139, 0.9248, 0.9381,  ..., 0.9015, 0.8980, 0.9191],\n",
      "        ...,\n",
      "        [0.8139, 0.9248, 0.9381,  ..., 0.9015, 0.8980, 0.9191],\n",
      "        [0.8139, 0.9248, 0.9381,  ..., 0.9015, 0.8980, 0.9191],\n",
      "        [0.8139, 0.9248, 0.9381,  ..., 0.9015, 0.8980, 0.9191]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor(19446.7949, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "19446.794921875\n",
      "epoch : 10/20, recon loss = 19446.79492188\n",
      "tensor([[111.,  89., 102.,  ...,  28.,  25.,  49.],\n",
      "        [164., 197., 190.,  ...,  42.,  36.,  64.],\n",
      "        [144., 148., 108.,  ...,  55.,  42.,  76.],\n",
      "        ...,\n",
      "        [100.,  68.,  78.,  ..., 139., 113., 108.],\n",
      "        [137., 136., 136.,  ...,  54.,  37.,  70.],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [ 96.,  68.,  79.,  ..., 142., 143., 136.]], device='cuda:0')\n",
      "tensor([[0.8680, 0.9516, 0.9608,  ..., 0.9345, 0.9345, 0.9477],\n",
      "        [0.8680, 0.9516, 0.9608,  ..., 0.9345, 0.9345, 0.9477],\n",
      "        [0.8680, 0.9516, 0.9608,  ..., 0.9345, 0.9345, 0.9477],\n",
      "        ...,\n",
      "        [0.8680, 0.9516, 0.9608,  ..., 0.9345, 0.9345, 0.9477],\n",
      "        [0.8680, 0.9516, 0.9608,  ..., 0.9345, 0.9345, 0.9477],\n",
      "        [0.8680, 0.9516, 0.9608,  ..., 0.9345, 0.9345, 0.9477]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor(19438.2363, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "19438.236328125\n",
      "epoch : 11/20, recon loss = 19438.23632812\n",
      "tensor([[127., 133., 134.,  ..., 106.,  99., 118.],\n",
      "        [156., 204., 100.,  ...,  19.,   0.,   7.],\n",
      "        [131., 137., 134.,  ...,  81.,  64.,  92.],\n",
      "        ...,\n",
      "        [119., 107., 113.,  ...,  59.,  48.,  78.],\n",
      "        [130., 121., 117.,  ..., 134., 165., 140.],\n",
      "        [128., 127., 130.,  ..., 128., 128., 129.]], device='cuda:0')\n",
      "tensor([[0.9117, 0.9700, 0.9761,  ..., 0.9584, 0.9599, 0.9676],\n",
      "        [0.9117, 0.9700, 0.9761,  ..., 0.9584, 0.9599, 0.9676],\n",
      "        [0.9117, 0.9700, 0.9761,  ..., 0.9584, 0.9599, 0.9676],\n",
      "        ...,\n",
      "        [0.9117, 0.9700, 0.9761,  ..., 0.9584, 0.9599, 0.9676],\n",
      "        [0.9117, 0.9700, 0.9761,  ..., 0.9584, 0.9599, 0.9676],\n",
      "        [0.9117, 0.9700, 0.9761,  ..., 0.9584, 0.9599, 0.9676]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor(19432.0586, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "19432.05859375\n",
      "epoch : 12/20, recon loss = 19432.05859375\n",
      "tensor([[112.,  81.,  83.,  ..., 187., 182., 164.],\n",
      "        [ 26.,  14.,  13.,  ...,  16.,  38.,  56.],\n",
      "        [125., 136., 147.,  ..., 145., 149., 132.],\n",
      "        ...,\n",
      "        [ 96.,  68.,  79.,  ..., 142., 143., 136.],\n",
      "        [127., 135., 132.,  ..., 123., 123., 126.],\n",
      "        [ 77.,  62.,  57.,  ...,  68.,  55.,  68.]], device='cuda:0')\n",
      "tensor([[0.9436, 0.9820, 0.9857,  ..., 0.9743, 0.9763, 0.9805],\n",
      "        [0.9436, 0.9820, 0.9857,  ..., 0.9743, 0.9763, 0.9805],\n",
      "        [0.9436, 0.9820, 0.9857,  ..., 0.9743, 0.9763, 0.9805],\n",
      "        ...,\n",
      "        [0.9436, 0.9820, 0.9857,  ..., 0.9743, 0.9763, 0.9805],\n",
      "        [0.9436, 0.9820, 0.9857,  ..., 0.9743, 0.9763, 0.9805],\n",
      "        [0.9436, 0.9820, 0.9857,  ..., 0.9743, 0.9763, 0.9805]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor(19427.9199, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "19427.919921875\n",
      "epoch : 13/20, recon loss = 19427.91992188\n",
      "tensor([[156., 204., 100.,  ...,  19.,   0.,   7.],\n",
      "        [128., 128., 128.,  ..., 141., 129., 125.],\n",
      "        [127., 135., 144.,  ..., 136., 143., 135.],\n",
      "        ...,\n",
      "        [138., 145., 136.,  ..., 146., 147., 140.],\n",
      "        [ 40.,  48.,  74.,  ..., 180., 142.,  67.],\n",
      "        [ 98.,  60.,  66.,  ...,  41.,  40.,  70.]], device='cuda:0')\n",
      "tensor([[0.9652, 0.9893, 0.9916,  ..., 0.9845, 0.9862, 0.9885],\n",
      "        [0.9652, 0.9893, 0.9916,  ..., 0.9845, 0.9862, 0.9885],\n",
      "        [0.9652, 0.9893, 0.9916,  ..., 0.9845, 0.9862, 0.9885],\n",
      "        ...,\n",
      "        [0.9652, 0.9893, 0.9916,  ..., 0.9845, 0.9862, 0.9885],\n",
      "        [0.9652, 0.9893, 0.9916,  ..., 0.9845, 0.9862, 0.9885],\n",
      "        [0.9652, 0.9893, 0.9916,  ..., 0.9845, 0.9862, 0.9885]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor(19425.3086, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "19425.30859375\n",
      "epoch : 14/20, recon loss = 19425.30859375\n",
      "tensor([[108., 111., 116.,  ..., 178., 157., 149.],\n",
      "        [128., 133., 129.,  ...,  84.,  76., 108.],\n",
      "        [143.,  89., 115.,  ..., 201., 201., 201.],\n",
      "        ...,\n",
      "        [126., 133., 143.,  ..., 114., 112., 125.],\n",
      "        [164., 197., 190.,  ...,  42.,  36.,  64.],\n",
      "        [129., 125., 123.,  ..., 129., 132., 131.]], device='cuda:0')\n",
      "tensor([[0.9789, 0.9936, 0.9950,  ..., 0.9906, 0.9920, 0.9932],\n",
      "        [0.9789, 0.9936, 0.9950,  ..., 0.9906, 0.9920, 0.9932],\n",
      "        [0.9789, 0.9936, 0.9950,  ..., 0.9906, 0.9920, 0.9932],\n",
      "        ...,\n",
      "        [0.9789, 0.9936, 0.9950,  ..., 0.9906, 0.9920, 0.9932],\n",
      "        [0.9789, 0.9936, 0.9950,  ..., 0.9906, 0.9920, 0.9932],\n",
      "        [0.9789, 0.9936, 0.9950,  ..., 0.9906, 0.9920, 0.9932]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor(19423.7266, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "19423.7265625\n",
      "epoch : 15/20, recon loss = 19423.72656250\n",
      "tensor([[112.,  81.,  83.,  ..., 187., 182., 164.],\n",
      "        [127., 131., 135.,  ..., 116., 117., 128.],\n",
      "        [126., 115., 114.,  ...,  74., 178., 147.],\n",
      "        ...,\n",
      "        [124., 111., 112.,  ...,  75., 117., 131.],\n",
      "        [100.,  77.,  81.,  ..., 152., 152., 152.],\n",
      "        [ 40.,  48.,  74.,  ..., 180., 142.,  67.]], device='cuda:0')\n",
      "tensor([[0.9873, 0.9962, 0.9971,  ..., 0.9943, 0.9953, 0.9960],\n",
      "        [0.9873, 0.9962, 0.9971,  ..., 0.9943, 0.9953, 0.9960],\n",
      "        [0.9873, 0.9962, 0.9971,  ..., 0.9943, 0.9953, 0.9960],\n",
      "        ...,\n",
      "        [0.9873, 0.9962, 0.9971,  ..., 0.9943, 0.9953, 0.9960],\n",
      "        [0.9873, 0.9962, 0.9971,  ..., 0.9943, 0.9953, 0.9960],\n",
      "        [0.9873, 0.9962, 0.9971,  ..., 0.9943, 0.9953, 0.9960]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor(19422.7891, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "19422.7890625\n",
      "epoch : 16/20, recon loss = 19422.78906250\n",
      "tensor([[ 82.,  88., 100.,  ..., 217., 244., 188.],\n",
      "        [136., 136., 143.,  ..., 162., 183., 172.],\n",
      "        [127., 127., 134.,  ..., 131., 131., 128.],\n",
      "        ...,\n",
      "        [111.,  91., 103.,  ...,  92.,  92.,  98.],\n",
      "        [119., 107., 127.,  ...,  99.,  82.,  81.],\n",
      "        [127., 135., 132.,  ..., 123., 123., 126.]], device='cuda:0')\n",
      "tensor([[0.9923, 0.9977, 0.9982,  ..., 0.9965, 0.9972, 0.9976],\n",
      "        [0.9923, 0.9977, 0.9982,  ..., 0.9965, 0.9972, 0.9976],\n",
      "        [0.9923, 0.9977, 0.9982,  ..., 0.9965, 0.9972, 0.9976],\n",
      "        ...,\n",
      "        [0.9923, 0.9977, 0.9982,  ..., 0.9965, 0.9972, 0.9976],\n",
      "        [0.9923, 0.9977, 0.9982,  ..., 0.9965, 0.9972, 0.9976],\n",
      "        [0.9923, 0.9977, 0.9982,  ..., 0.9965, 0.9972, 0.9976]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor(19422.2402, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "19422.240234375\n",
      "epoch : 17/20, recon loss = 19422.24023438\n",
      "tensor([[126., 112., 101.,  ..., 192., 203., 168.],\n",
      "        [125., 104., 104.,  ...,  76.,  79., 103.],\n",
      "        [128., 133., 137.,  ...,  86.,  81., 109.],\n",
      "        ...,\n",
      "        [134., 149., 142.,  ...,  52.,  49.,  81.],\n",
      "        [202., 204., 204.,  ..., 112., 134., 177.],\n",
      "        [128., 130., 130.,  ..., 105.,  99., 117.]], device='cuda:0')\n",
      "tensor([[0.9952, 0.9986, 0.9989,  ..., 0.9978, 0.9983, 0.9985],\n",
      "        [0.9952, 0.9986, 0.9989,  ..., 0.9978, 0.9983, 0.9985],\n",
      "        [0.9952, 0.9986, 0.9989,  ..., 0.9978, 0.9983, 0.9985],\n",
      "        ...,\n",
      "        [0.9952, 0.9986, 0.9989,  ..., 0.9978, 0.9983, 0.9985],\n",
      "        [0.9952, 0.9986, 0.9989,  ..., 0.9978, 0.9983, 0.9985],\n",
      "        [0.9952, 0.9986, 0.9989,  ..., 0.9978, 0.9983, 0.9985]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor(19421.9180, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "19421.91796875\n",
      "epoch : 18/20, recon loss = 19421.91796875\n",
      "tensor([[128., 128., 129.,  ..., 146., 137., 126.],\n",
      "        [124., 107., 109.,  ...,  80.,  73., 102.],\n",
      "        [130., 131., 132.,  ...,  71.,  52.,  84.],\n",
      "        ...,\n",
      "        [ 16.,   9.,  26.,  ...,  23.,  22.,  15.],\n",
      "        [111.,  83.,  88.,  ..., 182., 190., 165.],\n",
      "        [137., 161., 160.,  ..., 202., 205., 173.]], device='cuda:0')\n",
      "tensor([[0.9970, 0.9991, 0.9993,  ..., 0.9986, 0.9989, 0.9991],\n",
      "        [0.9970, 0.9991, 0.9993,  ..., 0.9986, 0.9989, 0.9991],\n",
      "        [0.9970, 0.9991, 0.9993,  ..., 0.9986, 0.9989, 0.9991],\n",
      "        ...,\n",
      "        [0.9970, 0.9991, 0.9993,  ..., 0.9986, 0.9989, 0.9991],\n",
      "        [0.9970, 0.9991, 0.9993,  ..., 0.9986, 0.9989, 0.9991],\n",
      "        [0.9970, 0.9991, 0.9993,  ..., 0.9986, 0.9989, 0.9991]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor(19421.7266, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "19421.7265625\n",
      "epoch : 19/20, recon loss = 19421.72656250\n",
      "tensor([[127., 132., 139.,  ..., 123., 122., 128.],\n",
      "        [127., 128., 135.,  ..., 132., 131., 128.],\n",
      "        [137., 147., 144.,  ...,  81.,  65.,  85.],\n",
      "        ...,\n",
      "        [128., 128., 128.,  ..., 141., 129., 125.],\n",
      "        [130., 132., 128.,  ...,  71.,  67., 105.],\n",
      "        [111.,  83.,  88.,  ..., 182., 190., 165.]], device='cuda:0')\n",
      "tensor([[0.9981, 0.9994, 0.9996,  ..., 0.9991, 0.9993, 0.9994],\n",
      "        [0.9981, 0.9994, 0.9996,  ..., 0.9991, 0.9993, 0.9994],\n",
      "        [0.9981, 0.9994, 0.9996,  ..., 0.9991, 0.9993, 0.9994],\n",
      "        ...,\n",
      "        [0.9980, 0.9994, 0.9996,  ..., 0.9991, 0.9993, 0.9994],\n",
      "        [0.9981, 0.9994, 0.9996,  ..., 0.9991, 0.9993, 0.9994],\n",
      "        [0.9981, 0.9994, 0.9996,  ..., 0.9991, 0.9993, 0.9994]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor(19421.6113, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "19421.611328125\n",
      "epoch : 20/20, recon loss = 19421.61132812\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    for batch_features, _ in train_loader:\n",
    "        # reshape mini-batch data to [N, 784] matrix\n",
    "        # load it to the active device\n",
    "        batch_features = batch_features.view(-1, 4096).to(device)\n",
    "        print(batch_features)\n",
    "        # reset the gradients back to zero\n",
    "        # PyTorch accumulates gradients on subsequent backward passes\n",
    "        optimizer.zero_grad()      \n",
    "        # compute reconstructions\n",
    "        outputs = model(batch_features)\n",
    "        print(outputs)\n",
    "        # compute training reconstruction loss\n",
    "        train_loss = criterion(outputs, batch_features)\n",
    "        print(train_loss)\n",
    "        # compute accumulated gradients\n",
    "        train_loss.backward()\n",
    "        # perform parameter update based on current gradients\n",
    "        optimizer.step()\n",
    "        # add the mini-batch training loss to epoch loss\n",
    "        loss += train_loss.item()\n",
    "    # compute the epoch training loss\n",
    "    loss = loss / len(train_loader)\n",
    "    print(loss)\n",
    "    # display the epoch training loss\n",
    "    print(\"epoch : {}/{}, recon loss = {:.8f}\".format(epoch + 1, epochs, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results:\n",
    "    if result == [1,0,0,0]:\n",
    "        print(\"Gryffindor\")\n",
    "    if result == [0,1,0,0]:\n",
    "        print(\"Hufflepuff\")\n",
    "    if result == [0,0,1,0]:\n",
    "        print(\"Ravenclaw\")\n",
    "    if result == [0,0,0,1]:\n",
    "        print(\"Slytherin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
